- Generative 모델 중 가장 간단한 형태 -> **오토 인코더, VAE**

### Autoencoder 소개
- 자동으로 인코딩!
	- 사물의 특징을 라벨 없이 학습
- 디코딩도 가능 -> 생성이 가능!
- 
- 즉 표현+생성

#### Autoencoder의 기본 구조
- 기본 형태
	- `입력 x` -> 네트워크 -> `입력 x`
	- 동일한 입력을 생성

- 중간에는 작은 네트워크 **병목**을 삽입
	- **입력 레이어가 매우 작은 차원으로 프로젝션**
	- 이를 디코더가 다시 복원 -> 원본
	- 디코더는 교체 가능

- 표현 학습
	- 고차원 데이터 -> 작은 데이터
	- 다시 원본 생성 = 핵심 요소를 담고 있음!

- 사람의 노력 = 레이블 필요 X

### Variatinal Autoencoder (VAE) 소개
- 오토인코더는 훌륭, 하지만 단점 有
	1. 학습하는 잠재공간 해석 X
	2. 학습된 잠재공간 연속적 X
	3. 생성된 데이터가 원본과 비슷 X or 품질 낮음

- VAE는..
	- 잠재공간 해석 쉬움, 연속적
	- 데이터 생성이 가능한 형태로 학습

#### 잠재공간 구성 : 점에서 영역으로
- 오토인코더는 잠재공간에 제약이 없음 -> 개별 포인트로 고정
	- 생성 위한 공간을 하나의 특정 포인트로 인식
- VAE는 이 잠재 공간을 해석하기 좋은 형태로 가정
	- **같은 숫자들의 데이터 포인트는 같은 공간에 모여 영역 전개**
- VAE는 가우시안 분포로 잠재공간을 나눔
	- 데이터 포인트를 연속적인 확률 분포로 나타냄

#### 가우시안분포를 이용한 공간 설정
- 두개의 파라미터로 확률 분포 그림
	1. **평균(뮤)** : 분포의 중앙값
	2. **표준편차(시그마)** : 분포의 폭

- 2차원 모델링
	- x 축 평균값
	- y 축 평균값
	- x 축 표준편차 
	- y 축 표준편차
- 4개의 파라미터로 2차원 공간의 특정한 영역 정의

- 2차원 잠재공간에 2개의 의미영역 디자인
	- 2개의 확률 분포 -> 2 * (2개의 평균, 2개의 분산) = 8
	- 총 8개의 파라미터로 4개의 가우시안 분포

- M개의 정규분포를 이용, 잠재공간을 디자인
- 연속적!

#### 가우시안 잠재공간과 샘플링의 연결
- 가우시안 잠재공간 <-> 오토인코더를 어떻게 연결?
	- **샘플링!**

- 오토인코더
	- 병목 벡터를 직접 연산 
	- 숫자 하나 -> 특징 정보를 **벡터**로 바꾸는 법을 학습
- VAE
	- 직접적인 벡터 학습 X
	- 가우시안 분포 결정하는 **평균**과 **표준편차**를 학습!
	- 즉, **확률 분포 자체를 학습**

- But! 디코더는 확률 분포가 아닌 **벡터**가 필요함
	- 확룰분포 -> 벡터를 얻는 방법?
	- **샘플링!**

- 인코더가 학습한 M개의 가우시안 확률분포를 **샘플링**
	- **하나의 표현 벡터 얻음** -> 디코더 -> 생성
- **피드포워드는 Okay**
	- **Backpropagation** 과정에선 샘플링에서 문제 발생
	- 샘플링은 랜덤성 포함 -> 인코더까지 역전파 Xㅜ 
	- **재파라미터화**로 해결!

#### VAE의 재파라미터화 트릭
- **샘플링 과정을 역전파가 가능하도록 변환**
- `z = μ + σ * ε`
	- ε는 N(0, 1) 따르는 무작위 변수

- 예: 숫자 7 재구성 과정
	1. **ε 생성**
		- 역전파 영향없는 상수
	2. **인코딩**
		- 숫자 7 이미지가 인코더를 통해 잠재 공간으로 변환
	3. **샘플링**
		- ε로 잠재변수 z 계산
		- z는 디코더의 입력
	4. **디코딩**
		- 잠재 변수 z로 숫자 7 이미지 재구성
	5. **손실 계산 및 역전파**
		- 재구성된 이미지 <-> 원본 이미지 손실(MSE/BCE)을 계산
		- 손실 정보로 역전파 통해 모델 업데이트(KL 발산은 생략)

- 요약
	1. 인코더가 확룰 분포 학습
	2. 디코더가 이 분포에서 샘플링된 벡터 사용 -> 이미지 재구성
	3. 재구성 이미지 <-> 원본 비교
	4. 더 좋은 확률 분포 모양(= 의미영역)을 학습

- ε는 고정, 역전파에 영향 X
- 모델이 무작위성을 효과적으로 처리, 안정적으로 학습

#### Log Variance의 사용
- 수치적 안정성 위해 Variance를 Log Variance로 사용
	- `σ = exp(log(σ)) = exp(0.5 * log_variance)`
- 로그 분산 사용 <-> 안사용
	- 샘플링된 데이터 분포는 동일
	- 분산 값이 항상 양수 보장 -> 안정성 ↑
	- 특히, 분산이 매우 작/클때 굿

### VAE의 손실함수
- VAE의 손실 함수는 두 가지 구성요소
	1. 재구성 손실
	2. 잠재 공간 정규화 손실
- 두 가지를 최소화해 잠재 표현을 학습

#### 1. 재구성 손실
- 디코더가 인코더의 잠재 표현을 사용하여 **얼마나 잘 재구성 하는지**
	- 입력 데이터 <-> 재구성 데이터 차이
	- **MSE**나 **Binary Cross-Entropy** 사용
		- `p(x|z)` : 잠재 변수 z로부터 재구성된 입력 데이터 x의 확률 분포

#### 2. 잠재 공간 정규화 손실
- VAE에서 가정한 의미공간이 실제 평균, 표준편차를 따라가느냐
	- 의도한 모양을 지키느냐
- **평균, 표준편차를 통해 배워지는 확률분포 q가 정규분포와 얼마나 가까운가**
	- **KL Divergence**

- **KL Divergence**
	- 하나의 확률 분포 q가 다른 확률 분포 p로 변하는데 필요한 정보량, 차이
	- 인코더 출력 잠재 변수 분포 `q(z|x)`와 사전 정의 가우시안 분포 `p(z)` 사이 차이를 측정
	- **모델의 잠재 공간이 정규 분포를 따르도록 하는것!**
	- 의도 <-> 학습 확률 분포 공간 유사도

![[K-DL.png]]

- 베타는 (재구성 손실 <-> 정규화 손실) 가중치 조절하는 하이퍼파라미터

- VAE의 잠재변수 공간은 해석 가능한 연속 공간
	- 평균, 표준편차 조정으로 영역 해석도 가능!

### 결론
- AE
	- 입력 데이터 -> 저차원 잠재 공간에 압축 -> 원래 데이터 복원
	- 공간 연속적 X, 생성된 데이터 다양성 제한
- VAE
	- 잠재 공간의 연속성을 보장
	- 의미있는 **데이터 포인트 샘플링**
- 샘플링 과정을 역전파 가능하게 변환 -> **재파라미터화**
- 손실 함수
	- **재구성 손실** : 입력<->재구성 차이 측정
	- **의미공간 모양 준수 손실** : 출력 분포 <-> 정규분포 차이 측정