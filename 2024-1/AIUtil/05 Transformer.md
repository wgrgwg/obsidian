- Transformer의 기능적 측면, 역할, 활용법
- 자연어 처리(NLP)에 강력함
	- 문장 의미 파악, 새로운 문장 생성
- 이미지 처리, 음성 인식도 응용

## 표현학습, Seq2Seq 리마인드
### 표현학습
- 세상을 기계가 이해할 수 있는 형태로 표현
- **데이터를 숫자, 벡터, 행렬 등 수학적 객체로 변환**

### Seq2Seq
- **'표현'을 바탕으로 하나의 시퀀스를 다른 시퀀스로 변환**
- 복수의 아이템들을 유의미한 벡터로 변환
	- 이 벡터는 시퀀스의 **의미**를 담고 있음

## Seq2Seq 구현체의 역사
### 초기 Seq2Seq 구현체 : RNN의 시대
- 초기 모델은 주로 순환 신경망(RNN)로 구현
- **시퀀스 데이터 처리에 강점**
- 문제점
	- 기울기 소실
	- 기울기 폭발
		- 신경망 깊어지거나, 시퀀스 길어질수록 그라디언트가 소실/폭발
	- 장기 의존성
		- 시퀀스의 앞/뒷 부분 사이의 연관성 학습 잘 X

### 해결책: Attention 메커니즘의 등장
- 이런 문제 해결 위해 어텐션 메커니즘 도입
- **모델이 입력 시퀀스의 중요 부분에 집중하여 필요한 정보를 선택적으로 추출**
- 전체 시퀀스 일괄 처리하는 대신 **관련성 높은 정보에 집중** -> 효율적 처리

### RNN + Attention: Seq2Seq 구현의 새로운 표준
- RNN+Attention은 s2s 구현체로 광범위하게 사용
	- 기계 번역, 음성 인식, 텍스트 요약

### Transformer: Attention만으로 구현된 새로운 시대
- Transformer : Seq2Seq에 새로운 패러다임 제시
	- 오로지 **Attention 매커니즘만 사용**
- 복잡한 RNN 구조 제거, **전체 시퀀스를 한번에 처리**
- 학습 속도 ↑, 더 긴 시퀀스/깊은 네트워크 처리 O
- 특정 도메인이나 문제에 국한 X, 다양한 문제 범용적 해결

## Transformer가 풀고자 하는 문제
- 트랜스 포머를 통해 해결하고자 한 문제들
1. **Seq2Seq를 위한 구조 제공** : 시퀀스 -> 다른 시퀀스 변환 목표인 문제에 효과적인 구조 제공
2. **다수 항목 인코딩** : 여러 항목을 하나의 의미 있는 숫자 집합으로 인코딩
3. **장기 의존성 인코딩** : RNN이나 LSTM도 어느 정도 해결하지만, 더 효율적으로 처리
4. **순차적 정보 인코딩** : Attention 자체는 순서 고려 X -> 이를 해결하기 위함
5. **빠른 인코딩** : 병렬 처리로 데이터 빠르게 인코딩
6. **단순한 인코딩** : 인코더+디코더를 위한 단순, 강력한 아키텍쳐 제공, 모델의 이해/확장, 적용 쉬움
7. **안정적인 학습** : 학습 중 발생하는 문제를 효과적으로 관리, 해결 -> 안정적 학습 환경 제공, 모델 성능 ↑

## Transformer가 제시한 해법
### 1. Seq2Seq를 위한 구조 제공
- RNN+Attention 구조를 대체
- Encoder+Decoder 구조
![[encoder+decoder.webp]]

### 2. 다수 항목 인코딩 + 장기 의존성 인코딩
- 현실 세계(4차원 공간, `x, y, z, 시간`)의 데이터는 시계열 데이터
	- 연속된 데이터 포인트로 구성
	- 이를 **하나의 벡터로 표현**하는 것이 중요
- Attention : 전체 데이터 포인트 고려, 각 데이터 포인트 중요도 파악 -> 전체 데이터 요약 벡터 생성

1. **멀티헤드 어텐션** : 동일한 데이터에 여러 어텐션 메커니즘 병렬 적용
	- 다양한 측면을 동시에 고려
	- 풍부하고 다차원적 데이터 표현 도출 
2. **셀프 어텐션** : 입력 데이터 내의 각 요소가 서로 어떻게 상호 작용하는 지를 모델링
	- 입력 시퀀스 내의 **각 요소 Query**와 **모든 요소들 Keys**의 관계를 평가
	- 장기 의존성 해결
3. **크로스 어텐션 :** Transformer 디코더에서 활용, 인코더에서 생성된 표현과 디코더의 현재 입력 사이의 상호 작용을 모델링
	- 인코더와 디코더 사이의 교차되는 정보 흐름 나타냄
	- 다수의 데이터 포인트 효과적 인코딩, 장기 의존성 포함 복잡한 시퀀스 구조 이해

### 3. 순차적 정보 인코딩
- 어텐션 그 자체는 입력 데이터의 순서 반영 X
- **입력 임베딩 단계에서 순서 정보를 추가 -> 포지셔널 인코딩**

- **포지셔널 인코딩** : 시퀀스 각 요소의 위치 정보를 임베딩 벡터에 추가
	- 위치 정보 `t`에 대응하는 임베딩 벡터 `E`를 반환
- 포지셔널 인코딩의 변형
	- **학습 가능한 포지셔널 인코딩** : pos encoding 값을 모델 학습 과정에서 스스로 학습
	- **상대적 포지셔널 인코딩** : 절대 위치가 아닌 상대 위치 사용.
		- 긴 시퀀스에 효과적, 문맥따라 유연하게 모델링

### 4. 빠른 인코딩
- 기존 RNN은 순차 처리 -> 시간 ↑↑
- 트랜스포머는 모든 요소를 병렬 처리 -> ↓↓
	- **각 요소는 다른 모든 요소와의 관계를 동시에 계산**

### 5. 단순한 아키텍쳐
- 단순한 구조 -> 소규모+대규모 모두 가능, 모델의 이해, 확장 적용 용이
- 인코더, 디코더의 각 레이어는 두 부분으로 나뉨
	- 멀티-헤드 어텐션
	- 포지셔널 와이즈 피드 포워드 네트워크
- 단순한 이유
	1. **모듈화** : 각 부분은 재사용 가능한 모듈로 구성
	2. **표준화된 레이어** : 각 레이어는 어텐션 + 피드 포워드 네트워크로 구성
	3. **병렬 처리**
	4. **유연성** : 다양한 크기, 형태의 입력 데이터 처리 가능

### 6. 안정적인 학습
1. **레이어 정규화** : 각 레이어 출력을 정규화해 파라미터 스케일과 이동 변화에 강인하게 만듦
	- 각 레이어의 입력을 평균+분산 사용해서 정규화
	- 수치적 불안정성 ↓, 모델이 더 빠르게 수렴
2. **잔차 연결** : 각 레이어는 입력값에 대한 출력값을 더하는 잔차 연결 포함
	- 정보 손실 방지, 소실된 기울기 완화
3. **어텐션 메커니즘** : 각 요소가 전체 시퀀스와 어떻게 상호 작용하는지 동적 결정


## Pretraining + Fine-Tuning 학습 패턴
- 트랜스포머가 강력한 이유 중 하나
- **==사전 학습 + 미세 조정 패턴의 일반화==**
- 복잡한 신경망 구조 직접 설계 X -> 사전 학습된 트랜스포머 모델 적용
- 즉, **대규모 사전 학습 모델 -> 분야에 특화된 소규모 데이터셋으로 미세 조정**


## 트랜스포머의 적용: 다양한 문제 해결을 위한 범용 아키텍처
- N21, N2N, N2M 등 다양한 형태의 입/출력 시퀀스 작업에 효과적임

### N21 문제의 적용
 - 여러 입력에서 단일 출력
- 예) 리뷰 문장의 감정 분석

### N2N 문제
- 입력 시퀀스와 동일한 길이의 출력 시퀀스 생성
- 예) 문장 품사 태깅

### N2M 문제의 적용
- 입력 시퀀스와 출력 시퀀스의 길이가 다름
- 예) 긴 입력문서 요약작업

## 정리
- 트랜스포머의 강점은 ==**유연성**==, 성능, 간결함, 확장성
	- 입력 데이터의 복잡한 패턴을 학습, 적절한 출력 생성
- **==사전 학습 + 미세 조정==**은 혁명