- 세상의 데이터 -> 숫자
	- **인코딩 Encoding**
	- **임베딩 Enbedding**
	- **특징 추출 Feature Extraction**
	- **벡터화 Vectorization**

## Data to Numbers 개요
- Data to Numbers는 크게 두 가지 방법
	1. **학습 기반 방법**
	2. **비학습 기반 방법**

1. **비학습기반 방법(Non-learning-based methods)**
	- 사람의 직관, 빈도수, 통계에 기반
	- 복잡한 매개변수 X
	- ex) 특정 단어 빈도수 기반 문서 벡터화
2. **학습 기반 방법(Learning-based methods)**
	- 대량의 데이터로 Converter(숫자 변환기)를 학습
	- 두 가지 방법 존재
		1. **처음부터(From Scratch)** 학습
			- 데이터 기반 -> 직접 모델 학습
			- 데이터 특성에 맞는 맞춤 변환기
		2. **사전 학습된 인코더(Pretrained Encoders)** 사용
			- 이미 학습된 인코더 활용

## Data to Numbers : 비학습기반 방법

- 신경망 같은 고급 기법 사용 X
![[non-learning-based-methods.webp]]
### 서수 인코딩(Ordinal Encoding)
- `Small : 1`, `Medium : 2`, `Large : 3`
- **순서가 중요한 범주형 데이터**

### 원-핫 인코딩(One-Hot Encoding)
- 각 **범주를 완전히 독립적인 변수로 취급**하여 인코딩
- 순서 없음
- `Red : [1,0,0]`, `Green : [0,1,0]`, `Blue : [0,0,1]`

### 이진 인코딩(Binary Encoding)
- **정수로 변환 후 이진수로 인코딩**
- 원-핫 인코딩에 비해 적은 공간 차지
- 큰 범주 데이터를 효율적 처리
- `Red : 01`, `Green : 10`, `Blue : 11`

### 라벨 인코딩(Label Encoding)
- **범주에 임의의 숫자를 할당**
- 간단, 직관적
- 모델이 숫자의 크기나 순서에 의미를 부여할 위험 O -> 주의
- `Cat : 1 or [1,0,0]`, `Dog : 2 or [0,1,0]`, `Fish : 3 or [0,0,1]`

### 빈도 인코딩(Frequency Encoding)
- **특정 항목이 데이터 셋에 나타나는 횟수 기반**
- 항목의 중요도나 빈도를 숫자로 직접 반영
- `Apple : 50번 등장 -> 50`

### 해시 인코딩(Hash Encoding)
- **해시 함수를 이용해서 범주형 값을 고정 크기 정수로 변환**
- 범주가 많을 때 좋음
- 해시 충돌 주의
- `Hash(New York) => 123`

### 이진 표현(Binary Representation)
- True/False를 1/0 으로 인코딩
- 단순, 직관적이며 조건부 특성에 좋음

### Bag-of-Words Encoding
- 텍스트 -> 숫자 벡터
- 문서 내 단어 출현 빈도 기반으로  벡터 생성

- 작동 방식
	1. **단어 사전 생성** : 분석할 텍스트에서 고유한 단어 목록을 만들어 단어 사전 구성
	2. **문서 벡터화** : 각 문서를 단어 사전에 있는 단어들의 출현 빈도롤 표현한 벡터로 변환

- 예시
	1. 단어 사전 : `{"A", "B}`
	2. 문장 벡터화 :
		- `"A B A" : ["A" : 2, "B" : 1] -> [2, 1] 
		- `"B A B : ["A" : 1, "B" : 2] -> [1, 2]`

- 텍스트 데이터를 간단/효과적으로 숫자 벡터로 변환

- 문제점
	- **단어의 순서, 문맥, 의미적 관계 고려 X**
- 복잡한 언어적 특성 모델링은 다른 방법 사용

### TF-IDF Encoding
- 텍스트 -> 숫자 벡터
- Term Frequency-Inverse Document Frequency
- 단어가 문서 집합 전체에서 얼마나 고유한지(중요한지)도 고려
	-> **중요한 단어 판별 가능**

- 작동 원리
	1. TF : 문서 내 단어 출현 빈도
	2. IDF : 단어의 희귀도

- 예시
	- 두 문서 모두에서 공통적으로 출현하는 단어는 중요도 ↓
	- 특정 문서에서만 출현하는 단어는 중요도 ↑
	- `특정 doc에서의 Freq / 모든 doc에서의 Freq`

- 중요한 단어 ↑, 자주 등장하고 문서의 주제와 관련 없는 단어(the, is) ↓
- **문서 내 단어의 상대적 중요성 파악에 유용**

## Dimension Reduction
- **차원 축소** : 텐서 형태의 데이터를 저차원의 공간에서 도출 -> 일종의 인코딩
- 신경망의 다중 퍼셉트론(MLP)도 차원 축소 수단 중 하나
- 훈련 기반, 비훈련 기반으로 구

### 주성분 분석(PCA)
- **데이터 분산이 최대화되는 방향을 찾아, 주성분을 기준으로 차원을 축소**
- 데이터의 **중요한 특징 추출, 노이즈 감소, 시각화 용이**
- 서로 상관관계 적은 변수 -> 주성분
	- 첫 번째 주성분 : 원본 데이터의 분산을 가장 많이 설명하는 축
	- 두 번째 주성분 : 첫 번째 주성분과 직각을 이루며 남은 분산을 가장 많이 설명하는 축
	- 이 과정을 반복하여, 중요 정보 보존 & 차원 축소

- 문제점
	- 비선형 데이터에는 부적합
- 분산 최대화 하는 방향 -> 분류나 클러스터링에는 중요X 일지도

### t-분산 확률적 이웃 포함(t-SNE)
- t-SNE(t-Distributed Stochastic Neighbor Embedding)
- 고차원 데이터를 저차원(2,3)으로 축소하여 시각화
- 고차원에서의 데이터 포인트 유사성 유지
- **복잡한 비선형 구조**에 유용
	- **클러스터 구조** 데이터 시각화에 강력

- 문제점
	- 계산 비용 ↑, 결과 해석 직관적 X, 결과의 재현성 ↓
- 하이퍼 파라미터의 선택이 결과에 큰 영향


## Data to Numbers : 학습기반 방법
![[learning-based-methods.webp]]

- **지도 학습** : **레이블이 지정된** 학습 데이터 사용하여 모델 학습
	- 입력-정답이 있는 경우
- **비지도 학습** : **레이블 없이 입력 데이터만으로** 모델 학습
	- 데이터의 숨겨진 구조나 패턴 찾기
- **자기 지도 학습** : **레이블이 필요 없는** 지도 학습
	- 입력 데이터에서 자동으로 레이블 생성
	- 입력 데이터의 내재된 구조 학습


### 지도 학습(Supervised Learning)
- 응용 문제를 성공적으로 해결하는 과정 -> 우수한 표현이 배워짐
- 예) 과일 인식기
	- 충분한 양의 Labeled Data 사용해 Classification
	- 다른 범주와 구분되는 충분한 차별화된 '표현'이 학습 과정에서 형성
	- 학습된 신경망 -> 과일 이미지 입력 -> 순전파 Feed-forward -> 숫자

- 장점
	- 직관적이고 성능이 우수
- 문제점
	1. **데이터셋 구성 비용**
	2. **레이블의 한계** : 사람이 부여한 레이블만 해당 
	3. **데이터 구분의 명확성** : 데이터가 명확히 구분돼야 함


### 비지도 학습(Unsupervised Learning)
#### Autoencoder
- 입력 데이터를 **저차원으로 압축(인코딩) -> 원래 차원으로 복원(디코딩)**하는 신경망 구조
	- 이 과정에서 중요한 특징 학습
	- 데이터의 차원 축소, 특성 학습, 생성 모델 등에 활용 

#### Denoising Autoencoder
- 오토인코더의 변형 -> **노이즈 제거 오토인코더**
- 입력 데이터에 노이즈 추가 -> 원본 데이터로 복원
	- 데이터의 중요한 특징 추출
	- 노이즈에 강함

#### 생성적 적대 신경망(GAN, Generative Adversarial Networks)
- 생성자와 판별자를 경쟁적으로 학습 시킴

- 생성자(Generator) : 무작위 잡음에서 시작, 실제와 유사한 데이터 생성
	- 목표 : 판별자 속이기
- 판별자(Discriminator) : 입력된 데이터를 진짜/가짜 판별
	- 목표 : 정확하게 구분

- 생성자-판별자 네트워크가 최적화 된 상태 -> **나쉬 균형(Nash Equilibruim)**
	- 한 쪽이 전략을 바꿔도, 다른 한 쪽이 이미 최적이므로 이득 X
- 판별자 - 50%확률로 진짜/가짜 예측 => **생성자가 실제 데이터의 표현을 배움** 


### 자기지도 학습(Self-Supervised Learning)
- **레이블이 명시적으로 제공되지 않는 데이터로부터 학습**
- 데이터 자체 내에서 학습 신호 찾기 
- 일부를 입력으로 사용 -> 나머지 부분 예측
	- 데이터의 내부구조와 패턴 학습 -> 숫자 표현 변환 가능

- 예) 이미지
	- 이미지 부분으로 다른 조각들의 위치 예상
	- 이미지의 외형, 표현 학습
- 예) 텍스트
	- Masked 언어 모델링
	- 일부 단어를 가려 모델이 그 단어를 예측
	- 데이터의 내부 구조와 맥락 이해

- 장점
	- 레이블된 데이터에 의존 X
	- 대량의 비구조화 데이터 활용 O
		- 일반화 특성 포착
	- 사전 학습된 표현을 필요로 하는 다운 스트림 작업에 사용
	- 데이터의 정보 손실 최소화, 효과적인 특성 추출