### 관점 1: 입-출력
- 전통적인 역할
	- 인코더 : 현실 세계의 정보를 잠재 변수 공간으로 프로젝션 다운
	- 디코더 : 잠재 변수 공간의 정보를 현실 세계의 정보로 프로젝션 업
- 트랜스포머 변형 구조 -> 인코더 온리 / 디코더 온리

#### 인코더
- **본질** : 현실 세계 데이터 -> 잠재 변수 공간
- **입력** : text, image, token
- **출력** : 벡터(숫자=회귀 or 클래스=분류)
- **기능** : `N개 Data` -> `1개 Data Vector`
- **특징** : 사람의 정보 -> 기계의 정보

#### 디코더
- **본질** : 오토인코더(벡터->디코딩) but GPT(입, 출 모두 N개 토큰)
- **입력** : tokens
- **출력** : tokens (프로젝션 업)
- **특징** : 아무 형태 -> 사람의 언어

- 언어 모델 중심
	- 인코더 : 심볼 -> 벡터
	- 디코더 : 심볼 -> 심볼


### 관점 2: 할 수 있는 일, 없는 일
#### 인코더
##### 가능
- 심볼 -> 벡터 작업 
	- 분류
	- 회귀분석 : `text` -> `연속적인 값`
	- 임베딩 : `text` -> `고차원 벡터`
	- 문서 유사도 측정
	- 감정 분석
	- 주제 모델링
	- 추출형 질의 응답

##### 불가능
- N개의 연속된 심볼 생성 불가능
- 언어 생성 불가능/매우 약한 성능

#### 디코더
#### 가능
- 심볼 -> 심볼 작업
	- 번역
	- 요약
	- 작문
	- 언어 생성
	- 채팅봇 개발
	- 문장 완성
	- 임베딩

#### 불가능?-기존
- 기존에 불가능하다고 여겨졌던 일
	- 분류
	- 회귀분석
	- 임베딩
	- 문서 유사도
	- 감정 분석
	- 주제 모델링

- **하지만, 디코더 기반 모델이 인코더 작업도 수행 가능!**
	- **분류** : GPT-1이 언어 모델링 사전 학습하고 미세 조정으로 성능 ↑
	- **회귀분석** : 자연어 처리와 관련된 작업 가능 ex. text -> 특정 속성 값 예측
	- **임베딩** : 생성형 언어모델을 후처리 -> LLVM2Vec 테크닉, GPT 3.5

### 관점 3: 훈련 방식, 비용, 효율성
#### 인코더 모델
##### 훈련 방식
- **Masked Language Model (MLM)**
	- 문장 내의 일부 단어 마스킹 -> 모델이 예측
- **Next Sentence Prediction (NSP)**
	- 두 문장의 연속 여부를 예측
- **Segment Classification**
	- 문서 내의 세그먼트 구분
- **Token Classification**
	- 각 토큰의 속성(품사 태깅 등) 예측

##### 훈련 비용
- MLM 방식 기준으로 계산
- `1000개` 토큰, 최대 토큰 길이 `100개` -> `10개` 시퀀스
- batch size `32` -> 한 번에 `32*100`개 입력 토큰 처리
- MLM rate `15%` 
- 한 배치당 예측 토큰 개수
	- 한 시퀀스당 100개 토큰 -> 15%니까 15개 토큰
	- 한 배치당 32개의 시퀀스 -> `32*15 = 480`

- 메모리, 계산 자원은 마스킹된 토큰의 수 + 시퀀스 길이에 비례

#### 디코더 모델
##### 훈련 방식
- Autoregressive LM
- `1000개` 토큰, 최대 토큰 길이 `100개` -> `10개` 시퀀스
- batch size `32`
- 한 배치 내에 32개의 시퀀스 -> 3200번의 예측 연산

- 메모리, 계산 자원은 시퀀스 길이 + 배치 크기에 비례

<hr>

- **인코더**
	- 문맥 내의 관계를 이해
	- MLM 방식 : 마스킹된 토큰의 수에 따라 비용 증가
- **디코더**
	- 시퀀스의 다음 토큰을 예측
	- ARLM : 시퀀스의 길이에 따라 비용 증가

- 분석의 주요 기준점
	- 같은 Data 노출시, 언어 모델 신경망의 예측 횟수
- 훈련 소모 자원
	- **데이터 1 유닛** : 언어 모델 훈련의 한 단계에서 사용되는 데이터 양
	- **최적화 1 유닛** :  예측값과 정답을 비교하여 objective 함수를 최적화하는 것

#### 인코더 모델 (MLM)
- 데이터 유닛 : `32*100` (32개의 시퀀스, 각 시퀀스 당 100개 토큰)
- 최적화 유닛
	- 한 배치 당 3200개 토큰 처리
	- 평균적으로 15%의 토큰 마스킹 -> **한 배치당 480개의 최적화 유닛 필요**

#### 디코더 모델 (ARLM)
- 데이터 유닛 : `32*100` (32개 시퀀스, 각 시퀀스 당 100개 토큰)
- 최적화 유닛
	- 각 시퀀스의 토큰마다 다음 토큰 예측 위해 피드포워드 + 백프로파게이션 수행
	- 한 배치당 `32*100` = 3200 번의 예측 연산 필요
	- 한 배치당 3200개의 토큰 각각에 대해 예측 <-> 정답 비교해 최적화
	- **한 배치당 3200개의 최적화 유닛**

- **학습 효율성 = 최적화 유닛 / 데이터 유닛**
	- 효율성은 디코더가 압도 !

<hr>

### 결론
- 세 가지 관점 존재
	- 입-출력
	- 전용모델의 가능성
	- 학습 방식 및 효율성

- 전략적 결정
- **디코더 모델이 좋다 !**
	- 같은 파라미터, 데이터 기준으로 **학습 효율성 ↑**
	- GPT-3, LLaMA, PaLM 모두 Decoder-only
	- 다만. 몇몇 태스크에서는 같은 성능에 많은 비용 필요
		- 임베딩 태스크, 문장 분류, 토큰 분류, 개체명 인식
- 상황따라 적절한 모델 선택 중요!
