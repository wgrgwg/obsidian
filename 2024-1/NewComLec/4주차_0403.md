## 04-1 분산 학습 시스템
- 통신의 단계
	1. 비트 전달
	2. 의미 전달

- 이미지 전달
	- 과거 : 이미지 -> 비트
	- 현재 : 이미지 -> 텍스트 의미 전달 -> 이미지 생성
__


		       AI/ML 
            /         \
	 분산 트레이닝		 분산 러닝
	     /                 \
`분산 컴퓨팅 -------------(무선)통신`

### 분산 트레이닝
- 모델 사이즈 up, 연산 부담도 up
- 하나의 GPU로 모델 학습 X

- 여러 GPU 활용
1. **Data Parallelism 데이터를 나눠서, 병렬화**
	- 병렬화 쉬움
	- 활용 어려움, 비용 ↑
2. **Model Parllelism 학습(순차적)을 나눠서**
	- 활용도 ↑
	- 연결 전달, 병렬화 어려움, 로드 밸런싱 문제
3. **Tensor Parallelsim 텐서(연산)를 쪼개서 뿌리자**

#### Data Parallelism
- **Parameter Server**
	- 높은 대역폭 요구 -> worker가 많을 경우 overload
		1. **중앙화 서버 없이 분산 학습**
		2. **기울기 압축**

### 분산 Learning
- 여러 기기로부터 얻은 정보
	- 중앙 서버로 -> Cloud
	- 기기에서 학습 -> On Device
- 여러 곳에 존재하는 데이터를 모으지 않고 학습
	- but, 개인의 데이터 충분 X, 자원도 X 

- 종류
	- **연합 학습**
	- **분할 학습**
	- **탈중앙 학습**

#### 연합 학습
- 서버가 있지만 데이터 전달은 X
- 학습한 결과만 수집해서 모델 업데이트
- 과제
	- 통신 문제
	- 평균 결과 -> 각각 모델 성능 저하
	- 결과도 데이터 침해 가능

#### 분할 학습
- 전체 모델을 쪼개서 디바이스 - 서버로 학습
- 학습의 중간 결과를 서버로 전달
- 장점
	- 모델 privacy ↑
	- 클라 연산 부담 ↓
	- 서버-클라 통신 오버헤드 ↓
	- 빠른 모델 수렴
- 단점
	- Label 유출
	- 느린 학습
	- 서버-클라 잦은 통신


## 04-2 Using Trees in Machine Learning
#### Decision Tree
- 분류 Classification 모델
- 리프 노드에 도달 -> Label 결정
- 답이 알려진 데이터 (학습 데이터) => 트리 구성
- 직관적이지만 Unstable함
	- -> 앙상블 메소드 사용 -> **Random Forest**

#### Random Forest
- 랜덤하게 구성 -> 병렬적
	- 서로 보완하려면? -> XGBoost

#### XGBoost
- Extreme Gradient Boosting
- 이전 트리의 예측 오차를 기반으로 새로운 트리를 훈련시켜 더함

#### Outlier Detection
- Outlier (Anomaly, abnormality, or novelty)
- **Outlier 이상치** : majority에서 벗어나는 데이터 샘플
- 활용
	- 사기꾼, 고장, 질병 detection
- 많은 방법이 있음 -> Tree-based
	- Deep SVDD : 딥러닝 + SVM

#### Isolation Forest
- iTree를 여러개 사용 -> 이상 데이터를 분리
- 이상 데이터는 루트 노드에 근접
- 아웃라이어 스코어 : 몇 번만에 리프 노드로 분리되는가?
	- iTree 들의 아웃라이어 스코어의 합을 계산



- 각 클라에서 로컬 데이터로 트리 앙상블 구성 -> 합침